{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0635285d-5589-4eeb-9091-9dd2143b3492",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d77bfb-132e-422c-9933-69d0df5685dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T14:54:16.587611Z",
     "start_time": "2024-05-04T14:54:14.898483Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = \"team9\"\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63347c73-2077-4d7e-b72e-0579fe675dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE team9_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "spark.sql(\"SELECT * FROM team9_projectdb.ctr_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a0875-0d8c-4f48-a65b-230e11c46d38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T14:54:49.571635Z",
     "start_time": "2024-05-04T14:54:49.560728Z"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc89ee-c475-47d1-b000-a800563a8870",
   "metadata": {},
   "source": [
    "# list Hive databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2631136-e462-4815-a807-d1bd4bc3f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())\n",
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db024f5a-4aac-429e-88da-906174ebf3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listTables(\"team9_projectdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f0630-f7b4-4dd5-8974-2a6208ba8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = spark.read.format(\"avro\").table('team9_projectdb.ctr_part')\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82e7da-9843-41ad-b023-8eed09dde9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr.printSchema()\n",
    "\n",
    "spark.sql(\"SELECT * FROM ctr_part WHERE user_id=287005\").show()\n",
    "\n",
    "spark.sql(\"SELECT AVG(product_category_1) FROM ctr_part;\").show()\n",
    "\n",
    "# spark.sql(\"SELECT * from ctr_part where product_category_2 is NULL;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1fe4-d2d3-4fb0-9533-59f61a917d05",
   "metadata": {},
   "source": [
    "# Specify the input and output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f48d1-6fde-4ff5-b7ce-4eed719a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the following features\n",
    "# Excluded 'product_category_2', 'city_development_index' because they have a lot of nulls\n",
    "# Excuded hiredate because it is given as practice to implement the cos_sin_transformation for the student\n",
    "features = ['session_id', 'DateTime', 'user_id', 'product', 'campaign_id',\n",
    "       'webpage_id', 'product_category_1',\n",
    "       'user_group_id', 'gender', 'age_level', 'user_depth', 'var_1']\n",
    "\n",
    "# The output/target of our model\n",
    "label = 'is_click'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bf565-af4e-49d4-bf45-ca1e1baf971f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376ccfd-3045-4c2d-9800-19ea2da702b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main = spark.read.format(\"avro\").table('team7_projectdb.main')\n",
    "\n",
    "# depts = spark.read.format(\"avro\").table('team7_projectdb.departments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a1c70-3549-484c-99b9-00631ccfbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c331c42-ec0f-41a7-9c43-b3621f278dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e05e6d-b953-4dd6-84ae-dbeac729dc76",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00984a8d-8516-47b6-891a-85711cb5f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#тут Фирас своей херней со своими данными страдает\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# # Remove the quotes before and after each string in job and ename columns.\n",
    "# emps = emps.withColumn(\"job\", F.translate(\"job\",\"'\",\"\"))\n",
    "# emps.show()\n",
    "# emps = emps.withColumn(\"ename\", F.translate(\"ename\",\"'\",\"\"))\n",
    "# emps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c00177-7c66-4341-9d9f-6ac1089889e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#тут тоже\n",
    "# emps = emps.select(features + [label]).na.drop()\n",
    "# emps = emps.withColumn(\"ename_job\", F.concat(F.col('ename'), F.lit(\"_\"), F.col('job')))\n",
    "# emps = emps.withColumnRenamed(\"sal\",\"label\")\n",
    "\n",
    "# emps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078657b6-8c1e-4898-a569-20f9e361e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# categoricalCols = ['deptno']\n",
    "# textCols = ['ename_job']\n",
    "# others = ['empno', 'mgr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ee491-87c6-490c-863a-2a2a50f0286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr_template = ctr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71bafe-c88b-47d2-accc-a9ca27f3a95d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967abaa7-94fa-46f3-bcdd-3c5693ad1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, avg, coalesce, date_format\n",
    "from pyspark.sql.functions import sin, cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896447a6-2a38-4f68-a45b-9d8d2cc37edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp = ctr\n",
    "from pyspark.sql.functions import col\n",
    "ctr1_temp = ctr1_temp.orderBy(col(\"DateTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403889c-2c5b-44dd-8e0b-91524ca15ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccb9ed-a3e5-4980-b8e6-7279aa961ece",
   "metadata": {},
   "source": [
    "играюсь с разделение даты на месяц, год, день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bed67c-bd7b-4eba-ba73-15c4559f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import col\n",
    "# # Convert the datetime column to date type\n",
    "# ctr_template = ctr_template.withColumn(\"datetime\", ctr_template[\"datetime\"].cast(\"Date\"))\n",
    "# # # Split dates to year, month, day# \n",
    "# ctr_template=ctr_template.withColumn(\"year\", year(ctr_template[\"datetime\"]).cast('int'))\n",
    "# ctr_template=ctr_template.withColumn(\"month\", month(ctr_template[\"datetime\"]).cast('int'))\n",
    "# ctr_template=ctr_template.withColumn(\"day\", dayofmonth(ctr_template[\"datetime\"]).cast('int'))\n",
    "# ctr_template.drop(\"datetime\")\n",
    "\n",
    "# ctr1_temp = ctr\n",
    "\n",
    "# ctr1_temp = ctr1_temp.orderBy(col(\"datetime\"))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"year\", date_format(\"datetime\", \"yyyy\").cast('int'))    \n",
    "ctr1_temp = ctr1_temp.withColumn(\"month\", date_format(\"datetime\", \"MM\").cast('int')) \n",
    "ctr1_temp = ctr1_temp.withColumn(\"day\", date_format(\"datetime\", \"dd\").cast('int'))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"hour\", date_format(\"datetime\", \"HH\").cast('int'))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"minute\", date_format(\"datetime\", \"mm\").cast('int'))\n",
    "ctr1_temp = ctr1_temp.drop(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9cccb-ae1e-4d01-924b-81817f259914",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.select('year').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e68541-9e51-463e-b893-f57dc571c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.select('month').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83b004-3258-4aa4-a5b8-f41b06caeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6250f-d5c9-4e2f-8592-3895f7dcfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode cyclical month and days\n",
    "#the columns 'year' and 'month' have only one distinct value. We can remove them from the database\n",
    "\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"month_sin\", sin(2 * math.pi * ctr1_temp.month / 12))\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"month_cos\", cos(2 * math.pi * ctr1_temp.month / 12))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"day_sin\", sin(2 * math.pi * ctr1_temp.day / 31))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"day_cos\", cos(2 * math.pi * ctr1_temp.day / 31))\n",
    "ctr1_temp = ctr1_temp.drop(*[\"month\", \"day\", \"year\"])\n",
    "\n",
    "# Encode cyclical hours and minutes\n",
    "\n",
    "ctr1_temp = ctr1_temp.withColumn(\"hour_sin\", sin(2 * math.pi * ctr1_temp.hour / 24))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"hour_cos\", cos(2 * math.pi * ctr1_temp.hour / 24))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"minute_sin\", sin(2 * math.pi * ctr1_temp.minute / 60))\n",
    "ctr1_temp = ctr1_temp.withColumn(\"minute_cos\", cos(2 * math.pi * ctr1_temp.minute / 60))\n",
    "ctr1_temp = ctr1_temp.drop(*[\"hour\", \"minute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec0345-b940-429e-92fa-fe05e039db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dc258-ca9a-4822-8390-290356d750ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de4ba8-d7cf-404d-b786-b6a0e36a7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f298e8-1ab8-4123-bf13-c92d5a630d91",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e618ace-e849-4890-944e-05631230a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr1_temp = ctr1_temp.withColumn(\"year\", date_format(\"datetime\", \"yyyy\").cast('int'))    \n",
    "# ctr1_temp = ctr1_temp.withColumn(\"month\", date_format(\"datetime\", \"MM\").cast('int')) \n",
    "# ctr1_temp = ctr1_temp.withColumn(\"day\", date_format(\"datetime\", \"DD\").cast('int')) \n",
    "# ctr1_temp=ctr1_temp.drop(\"datetime\")\n",
    "# ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87063fc5-b78d-4a72-a75c-2d25f7394483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec38f4-2fed-403e-b9f1-1398192fac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalCols = ['product', 'gender']\n",
    "# textCols = ['ename_job']\n",
    "#fill missing values\n",
    "nanCols = ['user_group_id', 'user_depth', 'age_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a7fe8-9f82-43f8-9fa2-dd88384545b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table\n",
    "# Fill missing values with median\n",
    "imputer_median = Imputer(\n",
    "    inputCols=nanCols, outputCols=[\"{}_imputed\".format(c) for c in nanCols]\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "# Add imputation cols to df\n",
    "ctr1_temp = imputer_median.fit(ctr1_temp).transform(ctr1_temp)\n",
    "ctr1_temp = ctr1_temp.drop(*[\"user_group_id\", \"user_depth\",\"age_level\"])\n",
    "\n",
    "# Fill missing values with mode\n",
    "# imputer_mode = Imputer(\n",
    "#     inputCols=[\"gender\"], outputCols=[\"gender_imputed\"]).setStrategy(\"mode\")\n",
    "# # ctr1_temp.fillna(\"Male\", subset=['gender'])\n",
    "\n",
    "# # Add imputation cols to df\n",
    "# ctr1_temp = imputer_mode.fit(ctr1_temp).transform(ctr1_temp)\n",
    "# ctr1_temp = ctr1_temp.drop(\"gender\")\n",
    "\n",
    "# # Функция для расчета моды\n",
    "# def fill_with_mode(df, column):\n",
    "#     mode = df.groupBy(column).count().orderBy(col(\"count\").desc()).collect()[0][0]\n",
    "#     df = df.withColumn(column, when(col(column).isNull(), mode).otherwise(col(column)))\n",
    "#     return df\n",
    "\n",
    "# def fill_with_mode(df, column):\n",
    "#     mode_value = df.groupBy(column).count().orderBy(col(\"count\").desc()).first()[0]\n",
    "#     return df.fillna({column: mode_value})\n",
    "\n",
    "# Заполните пропущенные значения модой\n",
    "# ctr1_temp = fill_with_mode(ctr1_temp, \"gender\")\n",
    "# mode_value = ctr1_temp.groupBy(\"gender\").count().orderBy(col(\"count\").desc()).first()[0]\n",
    "# ctr1_temp = ctr1_temp.fillna(mode_value, \"gender\")\n",
    "\n",
    "\n",
    "# Замена пустых значений (например, пустых строк) на указанное значение\n",
    "ctr1_temp = ctr1_temp.withColumn(\"gender\", when(ctr1_temp[\"gender\"] == \"NaN\", 'Male').otherwise(ctr1_temp[\"gender\"]))\n",
    "#drop these columns\n",
    "ctr1_temp = ctr1_temp.drop(*[\"product_category_2\", \"city_development_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bb19c-764e-46f7-b9f6-bd23c83a0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr1_temp = ctr1_temp.drop(*[\"product_category_2\", \"city_development_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213550f-107d-4caa-aff1-4e219ad990eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96178e9-dd03-48ff-911b-f5079c219c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Проверка на наличие значений NaN в DataFrame\n",
    "ctr1_temp.select([count(when(isnan(c), c)).alias(c) for c in ctr1_temp.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c31f8-a43f-4a77-84fb-89b8a120fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexers_main = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in encode_main]\n",
    "# pipeline = Pipeline(stages=indexers_main)\n",
    "# main = pipeline.fit(main).transform(main).drop(*encode_main + [\"id\"])\n",
    "\n",
    "#Encode categorical columns\n",
    "indexers_temp = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categoricalCols]\n",
    "pipeline = Pipeline(stages=indexers_temp)\n",
    "ctr1_temp = pipeline.fit(ctr1_temp).transform(ctr1_temp).drop(*categoricalCols + [\"session_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c5a18-4125-461a-b845-e857151e9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d7710-5f37-4204-9275-4614f5505079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr1_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aaca91-568a-47dd-b78c-764032cff675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#and t[0] != 'month_cos' and t[0] != 'day_cos'\n",
    "numeric_features = [t[0] for t in ctr1_temp.dtypes if (t[0] != 'is_click' and t[0] != 'hour_cos' and t[0] != 'minute_cos') and (t[1] == 'int' or t[1] == 'float'or t[1] == 'double')]\n",
    "ctr1_temp.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ba52e-e7ea-4956-a165-11e3e67bb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_data = ctr1_temp.select(numeric_features).toPandas()\n",
    "# axs = pd.plotting.scatter_matrix(numeric_data, figsize=(12, 12));\n",
    "# n = len(numeric_data.columns)\n",
    "\n",
    "# for i in range(n):\n",
    "#     v = axs[i, 0]\n",
    "#     v.yaxis.label.set_rotation(0)\n",
    "#     v.yaxis.label.set_ha('right')\n",
    "#     v.set_yticks(())\n",
    "#     h = axs[n-1, i]\n",
    "#     h.xaxis.label.set_rotation(90)\n",
    "#     h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cebfcf-d13d-4c55-9da1-d3898eeb5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr1_temp.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d546fea-4273-439f-b073-e77b124db751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode cyclical month and days\n",
    "# ctr1_temp[\"month_sin\"] = sin(2 * math.pi * ctr1_temp.month / 12)\n",
    "# ctr1_temp[\"month_cos\"] = cos(2 * math.pi * ctr1_temp.month / 12)\n",
    "# ctr1_temp[\"day_sin\"] = sin(2 * math.pi * ctr1_temp.day / 31)\n",
    "# ctr1_temp[\"day_cos\"] = cos(2 * math.pi * ctr1_temp.day / 31)\n",
    "\n",
    "# # Encode cyclical month and days\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"month_sin\", sin(2 * math.pi * ctr1_temp.month / 12))\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"month_cos\", cos(2 * math.pi * ctr1_temp.month / 12))\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"day_sin\", sin(2 * math.pi * ctr1_temp.day / 31))\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"day_cos\", cos(2 * math.pi * ctr1_temp.day / 31))\n",
    "# ctr1_temp = ctr1_temp.drop(*[\"month\", \"day\"])\n",
    "\n",
    "\n",
    "# Assemble all features into single column\n",
    "assembler = VectorAssembler(inputCols=[i for i in ctr1_temp.schema.names if i != \"is_click\"], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[assembler])\n",
    "# ctr1_temp = pipeline.fit(ctr1_temp).transform(ctr1_temp)\n",
    "# ctr1_temp = ctr1_temp.select([\"label\", \"features\"])\n",
    "# # .withColumnRenamed(\"sales\", \"label\")\n",
    "\n",
    "# # Display final table\n",
    "# ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02360b53-6842-4781-893d-42ba51d62d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c3716-c47e-4cc5-8490-2610cb85b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp = pipeline.fit(ctr1_temp).transform(ctr1_temp)\n",
    "# ctr1_temp = ctr1_temp.select([\"label\", \"features\"])\n",
    "# # .withColumnRenamed(\"sales\", \"label\")\n",
    "\n",
    "# # Display final table\n",
    "# ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da894b-e3c4-40a4-90d5-459398bb4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1_temp = ctr1_temp.select([\"is_click\", \"features\"]).withColumnRenamed(\"is_click\", \"label\")\n",
    "\n",
    "# Display final table\n",
    "ctr1_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389febd-20de-4346-9d16-57dba4cf56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from pyspark.ml import Pipeline\n",
    "# # from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "# # from pyspark.sql.functions import col\n",
    "# # from pyspark.ml.feature import Imputer\n",
    "\n",
    "# categoricalCols = ['product', 'gender']\n",
    "# # textCols = ['ename_job']\n",
    "# #fill missing values\n",
    "# nanCols = ['user_group_id', 'user_depth', 'age_level']\n",
    "\n",
    "# # Since the tokenizer only return tokens separated by white spaces, I used RegexTokenizer to tokenize by '_'\n",
    "# # Then created word2Vec model\n",
    "\n",
    "# # tokenizer = Tokenizer(inputCol=\"ename\", outputCol=\"ename_tokens\")\n",
    "# # emps_tok = tokenizer.transform(emps)\n",
    "# # tokenizer = RegexTokenizer(inputCol=textCols[0], outputCol=\"ename_job_tokens\", pattern=\"_\")\n",
    "# # emps_tok = tokenizer.transform(emps)\n",
    "# # emps_tok.show()\n",
    "\n",
    "# # word2Vec = Word2Vec(vectorSize=5, seed=42, minCount=1, inputCol=\"ename_job_tokens\", outputCol=\"ename_enc\")\n",
    "# # word2VecModel = word2Vec.fit(emps_tok)\n",
    "# # print(word2VecModel)\n",
    "\n",
    "# # emps_tok = word2VecModel.transform(emps_tok)\n",
    "# # emps_tok.show()\n",
    "\n",
    "# # Adding the encoded ename_job to the list of other columns\n",
    "# # others += [ename_enc]\n",
    "\n",
    "\n",
    "# # Create String indexer to assign index for the string fields where each unique string will get a unique index\n",
    "# # String Indexer is required as an input for One-Hot Encoder \n",
    "# # We set the case as `skip` for any string out of the input strings\n",
    "# # indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categoricalCols ]\n",
    "\n",
    "# # Encode the strings using One Hot encoding\n",
    "# # default setting: dropLast=True ==> For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\n",
    "# # encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "\n",
    "# # This will concatenate the input cols into a single column.\n",
    "# # assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + others, outputCol= \"features\")\n",
    "\n",
    "# # You can create a pipeline to use only a single fit and transform on the data.\n",
    "# # pipeline = Pipeline(stages=[tokenizer, word2Vec] + indexers + encoders + [assembler])\n",
    "\n",
    "\n",
    "# # Fit the pipeline ==> This will call the fit functions for all transformers if exist\n",
    "# # model=pipeline.fit(emps)\n",
    "# # Fit the pipeline ==> This will call the transform functions for all transformers\n",
    "# # data = model.transform(emps)\n",
    "\n",
    "# # data.show()\n",
    "\n",
    "\n",
    "# #Сашин код\n",
    "# ###################################################\n",
    "# # # Table main\n",
    "# # # Encode categorical features in table \n",
    "# # mainindexers_main = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in encode_main]\n",
    "# # pipeline = Pipeline(stages=indexers_main)main = pipeline.fit(main).transform(main).drop(*encode_main + [\"id\"])\n",
    "# # # Table oil\n",
    "# # # Fill missing values in oil table with average of \n",
    "# # neighborswindow = Window.rowsBetween(-1, 1)\n",
    "# # oil = oil.withColumn(\"avg_dcoilwtico\", avg(oil[\"dcoilwtico\"]).over(window))oil = oil.withColumn(\"dcoilwtico\", coalesce(oil[\"dcoilwtico\"], oil[\"avg_dcoilwtico\"]))\n",
    "# # oil = oil.drop(\"avg_dcoilwtico\")\n",
    "# # # Table holidays_events# Convert boolean \"transferred\" column to integer column\n",
    "# # hol_events = hol_events.withColumn('transferred', when(hol_events.transferred==True, 1).otherwise(0))\n",
    "# # # Encode categorical features in table \n",
    "# # holidays_eventsindexers_hol_events = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in encode_hol_events]\n",
    "# # pipeline = Pipeline(stages=indexers_hol_events)hol_events = pipeline.fit(hol_events).transform(hol_events).drop(*(encode_hol_events + [\"description\", \"id\"]))\n",
    "# # # Table stores\n",
    "# # # Encode categorical features in table \n",
    "# # storesindexers_stores = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in encode_stores]\n",
    "# # pipeline = Pipeline(stages=indexers_stores)stores = pipeline.fit(stores).transform(stores).drop(*encode_stores)\n",
    "\n",
    "# # # We delete all features and keep only the features and label columns\n",
    "# # data = data.select([\"features\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ctr1_temp = ctr\n",
    "\n",
    "# # Table main\n",
    "# # Encode categorical features in table main\n",
    "# #Это безобразие у меня сработало!!!!\n",
    "# indexers_temp = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categoricalCols]\n",
    "# pipeline = Pipeline(stages=categoricalCols)\n",
    "# ctr1_temp = pipeline.fit(ctr1_temp).transform(ctr1_temp).drop(*categoricalCols + [\"session_id\"])\n",
    "# #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Table\n",
    "# # Fill missing values in oil table with median\n",
    "\n",
    "# imputer = Imputer(\n",
    "#     inputCols=nanCols, outputCols=[\"{}_imputed\".format(c) for c in nanCols]\n",
    "#     ).setStrategy(\"median\")\n",
    "\n",
    "# # Add imputation cols to df\n",
    "# ctr1_temp = imputer.fit(ctr1_temp).transform(ctr1_temp)\n",
    "\n",
    "# # Замена пустых значений (например, пустых строк) на указанное значение\n",
    "# ctr1_temp = ctr1_temp.withColumn(\"gender\", when(ctr1_temp[\"gender\"] == \"NaN\", 'Male').otherwise(ctr1_temp[\"gender\"]))\n",
    "# #drop these columns\n",
    "# ctr1_temp = ctr1_temp.drop(*[\"product_category_2\", \"city_development_index\"])\n",
    "\n",
    "# # window = Window.rowsBetween(-1, 1)\n",
    "# # oil = oil.withColumn(\"avg_dcoilwtico\", avg(oil[\"dcoilwtico\"]).over(window))\n",
    "# # oil = oil.withColumn(\"dcoilwtico\", coalesce(oil[\"dcoilwtico\"], oil[\"avg_dcoilwtico\"]))\n",
    "# # oil = oil.drop(*[\"avg_dcoilwtico\", \"id\"])\n",
    "\n",
    "# # Table holidays_events\n",
    "# # Convert boolean \"transferred\" column to integer column\n",
    "# # hol_events = hol_events.withColumn('transferred', when(hol_events.transferred, 1).otherwise(0)).drop(\"id\")\n",
    "\n",
    "# # Encode categorical features in table holidays_events\n",
    "# # indexers_hol_events = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in encode_hol_events]\n",
    "# # pipeline = Pipeline(stages=indexers_hol_events)\n",
    "# # hol_events = pipeline.fit(hol_events).transform(hol_events).drop(*(encode_hol_events + [\"description\", \"id\"]))\n",
    "\n",
    "# # Table stores\n",
    "# # Encode categorical features in table stores\n",
    "# # stores = stores.withColumnRenamed(\"type\", \"type_store\")\n",
    "# # indexers_stores = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in encode_stores]\n",
    "# # pipeline = Pipeline(stages=indexers_stores)\n",
    "# # stores = pipeline.fit(stores).transform(stores).drop(*encode_stores)\n",
    "\n",
    "# # Table transactions\n",
    "# # transactions = transactions.drop(\"id\")\n",
    "\n",
    "# # Join tables\n",
    "# # total_data = main. \\\n",
    "# #     join(oil, on=\"dates\"). \\\n",
    "# #     join(hol_events, on=\"dates\"). \\\n",
    "# #     join(transactions, on=[\"dates\", \"store_nbr\"]). \\\n",
    "# #     join(stores, on=\"store_nbr\")\n",
    "\n",
    "# # Split dates to year, month, day\n",
    "# # total_data = total_data. \\\n",
    "# #     withColumn(\"year\", date_format(\"dates\", \"yyyy\").cast('int')). \\\n",
    "# #     withColumn(\"month\", date_format(\"dates\", \"MM\").cast('int')). \\\n",
    "# #     withColumn(\"day\", date_format(\"dates\", \"DD\").cast('int')). \\\n",
    "# #     drop(\"dates\")\n",
    "\n",
    "# # Encode cyclical month and days\n",
    "# ctr1_temp[\"month_sin\"] = sin(2 * math.pi * ctr1_temp.month / 12)\n",
    "# ctr1_temp[\"month_cos\"] = cos(2 * math.pi * ctr1_temp.month / 12)\n",
    "# ctr1_temp[\"day_sin\"] = sin(2 * math.pi * ctr1_temp.day / 31)\n",
    "# ctr1_temp[\"day_cos\"] = cos(2 * math.pi * ctr1_temp.day / 31)\n",
    "\n",
    "# # Assemble all features into single column\n",
    "# assembler = VectorAssembler(inputCols=[i for i in ctr1_temp.schema.names if i != \"label\"], outputCol=\"features\")\n",
    "# pipeline = Pipeline(stages=[assembler])\n",
    "# ctr1_temp = pipeline.fit(ctr1_temp).transform(ctr1_temp)\n",
    "# ctr1_temp = ctr1_temp.select([\"is_click\", \"features\"]).withColumnRenamed(\"is_click\", \"label\")\n",
    "\n",
    "# # Display final table\n",
    "# ctr1_temp.show()\n",
    "\n",
    "# ###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "# # from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# # Automatically identify categorical features, and index them.\n",
    "# # We specify maxCategories so features with > 4\n",
    "# # distinct values are treated as continuous.\n",
    "# # featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "# # transformed = featureIndexer.transform(data)\n",
    "\n",
    "# # Display the output Spark DataFrame\n",
    "# # transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd3df2-8f31-42ed-8f0f-7dfcb795a5e8",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca00fb-a991-47d0-8d09-75e5604ed694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split the data into 60% training and 40% test (it is not stratified)\n",
    "(train_data, test_data) = ctr1_temp.randomSplit([0.6, 0.4], seed = 10)\n",
    "\n",
    "def run(command):\n",
    "    import os\n",
    "    # return os.popen(command).read()\n",
    "    #поменять потом, когда соберу весь пайплайн\n",
    "    return os.popen(\"cd ..\\n\" + command).read()\n",
    "\n",
    "\n",
    "train_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fecd52-189d-45af-841c-47819b311918",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daffa79-5f41-4e78-aa1a-c914f8b3f1f5",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892ef1c-f7c4-4acd-8d7d-5630f0823c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Create Linear Classification Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_lr = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1396-77d4-4dd8-8777-d0e6fcbd629f",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0f1ff-a175-4e00-ac1b-a5476dd7dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e97403-59a0-4ec4-b044-678ba6aa2b66",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492ad2e-15bc-4ed6-97e1-b60a32649297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670c975-566b-4bef-8a3e-c82bf5a171a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#надо дропнуть никто не шарит \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(model_lr.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67409f2c-2c26-4cf0-82d2-816d440417b5",
   "metadata": {},
   "source": [
    "Summarize the model over the training set, we can also obtain the ROC Receiver-Operating Characteristic) and the Area under ROC (areaUnderROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc0cfd-f997-4e9e-8599-94db5967ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = model_lr.summary\n",
    "lrROC = trainingSummary.roc.toPandas()\n",
    "\n",
    "plt.plot(lrROC['FPR'],lrROC['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab293a-77df-4957-bdfe-6aeac6bd4f03",
   "metadata": {},
   "source": [
    "Precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9d681-3546-4cf2-90a2-d1c4a0f17f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b166ca-8b62-4995-adca-129021dd6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
    "\n",
    "# lrEval = BinaryClassificationEvaluator()\n",
    "# evaluator1_area_ROC = lrEval.evaluate(predictions)\n",
    "# print('Area Under ROC on test data = {}'.format(evaluator1_area_ROC))\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "lrEval = BinaryClassificationEvaluator(labelCol='label')\n",
    "# evaluator2_area_ROC = lrEval.evaluate(predictions)\n",
    "auroc = lrEval.evaluate(predictions,{lrEval.metricName:'areaUnderROC'})\n",
    "aupr = lrEval.evaluate(predictions,{lrEval.metricName:'areaUnderPR'})\n",
    "\n",
    "print(f\"AUROC: {auroc}\")\n",
    "print(f\"AUPR: {aupr}\")\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator\n",
    "evaluator1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute accuracy, precision, recall, and F1 score\n",
    "accuracy1 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"accuracy\"})\n",
    "precision1 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"weightedPrecision\"})\n",
    "recall1 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"weightedRecall\"})\n",
    "f11 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"f1\"})\n",
    "\n",
    "# Print the metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy1}\")\n",
    "print(f\"Precision: {precision1}\")\n",
    "print(f\"Recall: {recall1}\")\n",
    "print(f\"F1 Score: {f11}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e075-c37c-492a-b894-2f64d4dea72b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04437d-2bf9-4803-83cc-b7ed17d311e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4388f9-db94-423c-879d-392c90041d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# grid = ParamGridBuilder()\n",
    "# grid = grid.addGrid(\n",
    "#                     model_lr.aggregationDepth, [2, 3, 4])\\\n",
    "#                     .addGrid(model_lr.regParam, np.logspace(1e-3,1e-1)\n",
    "#                     )\\\n",
    "#                     .build()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Создайте сетку параметров\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model_lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(model_lr.fitIntercept, [False, True]) \\\n",
    "    .addGrid(model_lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Создайте экземпляр CrossValidator\n",
    "cv = CrossValidator(estimator = lr, \n",
    "                    estimatorParamMaps = paramGrid, \n",
    "                    evaluator = lrEval,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "# Обучите модель на тренировочных данных\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel\n",
    "\n",
    "# Предскажите на тестовых данных\n",
    "# predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27e31c-959e-438c-bec0-1f109d7e2c15",
   "metadata": {},
   "source": [
    "## Best model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86fc18-eb29-4f03-bfef-ed3aa1daa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model1 = bestModel\n",
    "pprint(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f666a-f792-4b50-b38f-b39f9208ab25",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562da25-53d1-46dc-ac5d-68a6e9041927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model1 models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b220-54e1-49ce-802c-fc005d8f63c3",
   "metadata": {},
   "source": [
    "## Predict for test data using best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a05de-dcae-4bb0-8cf1-de760a62e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096936c5-a575-48b7-92d2-be22893e892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model1_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions.csv/*.csv > output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9a43d-80ce-4ec1-930f-671a240673d9",
   "metadata": {},
   "source": [
    "## Evaluate the best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cc0e6-7d0d-4030-a372-375e5e9b84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# rmse1 = evaluator1_rmse.evaluate(predictions)\n",
    "# r21 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "# print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse1))\n",
    "# print(\"R^2 on test data = {}\".format(r21))\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "lrEval1 = BinaryClassificationEvaluator(labelCol='label')\n",
    "# evaluator2_area_ROC = lrEval.evaluate(predictions)\n",
    "auroc1 = lrEval1.evaluate(predictions,{lrEval1.metricName:'areaUnderROC'})\n",
    "aupr1 = lrEval1.evaluate(predictions,{lrEval1.metricName:'areaUnderPR'})\n",
    "\n",
    "print(f\"AUROC: {auroc1}\")\n",
    "print(f\"AUPR: {aupr1}\")\n",
    "# lrEval = BinaryClassificationEvaluator()\n",
    "# evaluator1_area_ROC = lrEval.evaluate(predictions)\n",
    "# print('Area Under ROC on test data = {}'.format(evaluator1_area_ROC))\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator\n",
    "evaluator1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute accuracy, precision, recall, and F1 score\n",
    "accuracy1 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"accuracy\"})\n",
    "precision1 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"weightedPrecision\"})\n",
    "recall1 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"weightedRecall\"})\n",
    "f11 = evaluator1.evaluate(predictions, {evaluator1.metricName: \"f1\"})\n",
    "\n",
    "# Print the metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy1}\")\n",
    "print(f\"Precision: {precision1}\")\n",
    "print(f\"Recall: {recall1}\")\n",
    "print(f\"F1 Score: {f11}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308587c-3e52-41c6-ba7d-494d53cc45b1",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929b5a0-a159-4c94-ae56-086d3cf52df0",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8981a-e274-4ab7-9d03-fefdc068b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create Linear Regression Model\n",
    "gbt = GBTClassifier()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_gbt = gbt.fit(train_data)\n",
    "\n",
    "\n",
    "# from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# gbt = GBTClassifier()\n",
    "# gbtModel = gbt.fit(train)\n",
    "# gbtPreds = gbtModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c9a67-fe61-49fe-898f-017df0c4006b",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb95001-9d15-4da4-98df-2b87c0c39200",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_gbt.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94da6f-f913-45ec-a377-d9c42dacb075",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3508997-af5a-4882-8fd6-8d0a5be07ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "# r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "# print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "# print(\"R^2 on test data = {}\".format(r22))\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "lrEval2 = BinaryClassificationEvaluator(labelCol='label')\n",
    "# evaluator2_area_ROC = lrEval.evaluate(predictions)\n",
    "auroc2 = lrEval2.evaluate(predictions,{lrEval2.metricName:'areaUnderROC'})\n",
    "aupr2 = lrEval2.evaluate(predictions,{lrEval2.metricName:'areaUnderPR'})\n",
    "\n",
    "print(f\"AUROC: {auroc2}\")\n",
    "print(f\"AUPR: {aupr2}\")\n",
    "# print('Area Under ROC on test data = {}'.format(evaluator2_area_ROC))\n",
    "\n",
    "\n",
    "# Evaluate the model using a BinaryClassificationEvaluator\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "# auroc = evaluator.evaluate(prediction,{evaluator.metricName:'areaUnderROC'})\n",
    "# aupr = evaluator.evaluate(prediction,{evaluator.metricName:'areaUnderPR'})\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator\n",
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute accuracy, precision, recall, and F1 score\n",
    "accuracy2 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"accuracy\"})\n",
    "precision2 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"weightedPrecision\"})\n",
    "recall2 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"weightedRecall\"})\n",
    "f12 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"f1\"})\n",
    "\n",
    "# Print the metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy2}\")\n",
    "print(f\"Precision: {precision2}\")\n",
    "print(f\"Recall: {recall2}\")\n",
    "print(f\"F1 Score: {f12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2d06-39bd-4c71-b0cc-18a63d995949",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e50b9-5090-45ed-bfa7-7ddca0706176",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea6a22-9f7a-483b-818e-0b4e62f723e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(model_gbt.maxDepth, [2, 5, 10]).addGrid(model_gbt.maxBins, [20, 30]).build()\n",
    "\n",
    "cv = CrossValidator(estimator = gbt, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = lrEval2,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524320f-8764-4ed4-b8ff-9d0a43b2ee29",
   "metadata": {},
   "source": [
    "## Best model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccafef7f-5a51-4370-8572-447f06b92248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model2 = bestModel\n",
    "pprint(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3f34-cd0b-4e7e-a0ff-8670bee1e2ef",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bb094-67c3-4a8b-a52a-95a1f7c7bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model2 models/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220eee3-99d3-44cc-82ab-b941fd5e5e7c",
   "metadata": {},
   "source": [
    "## Predict for test data using best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53789d15-33a1-43b2-8e8e-df7627df5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df5365-5d86-407f-b71a-878fff80b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions.csv/*.csv > output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e394-a65a-4cb3-b9c8-1036fd3a443a",
   "metadata": {},
   "source": [
    "## Evaluate the best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83b832-0088-4e95-95fc-768bc13d70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "# r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "# print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "# print(\"R^2 on test data = {}\".format(r22))\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "lrEval2 = BinaryClassificationEvaluator(labelCol='label')\n",
    "# evaluator2_area_ROC = lrEval.evaluate(predictions)\n",
    "auroc2 = lrEval2.evaluate(predictions,{lrEval2.metricName:'areaUnderROC'})\n",
    "aupr2 = lrEval2.evaluate(predictions,{lrEval2.metricName:'areaUnderPR'})\n",
    "# print('Area Under ROC on test data = {}'.format(evaluator2_area_ROC))\n",
    "\n",
    "\n",
    "# Evaluate the model using a BinaryClassificationEvaluator\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "# auroc = evaluator.evaluate(prediction,{evaluator.metricName:'areaUnderROC'})\n",
    "# aupr = evaluator.evaluate(prediction,{evaluator.metricName:'areaUnderPR'})\n",
    "\n",
    "print(f\"AUROC: {auroc2}\")\n",
    "print(f\"AUPR: {aupr2}\")\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator\n",
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute accuracy, precision, recall, and F1 score\n",
    "accuracy2 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"accuracy\"})\n",
    "precision2 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"weightedPrecision\"})\n",
    "recall2 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"weightedRecall\"})\n",
    "f12 = evaluator2.evaluate(predictions, {evaluator2.metricName: \"f1\"})\n",
    "\n",
    "# Print the metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy2}\")\n",
    "print(f\"Precision: {precision2}\")\n",
    "print(f\"Recall: {recall2}\")\n",
    "print(f\"F1 Score: {f12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21d9a9-3f73-42ce-af59-352c3b3fffb4",
   "metadata": {},
   "source": [
    "# Third Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c1816-6147-40b5-a771-b556b6686297",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3584a-90e1-4d09-853d-66e30363fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import LinearRegression\n",
    "# # Create Linear Regression Model\n",
    "# lr = LinearRegression()\n",
    "\n",
    "# # Fit the data to the lr model\n",
    "# model_lr = lr.fit(train_data)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Create Random Forest Model\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "# Fit the data to the rf model\n",
    "rfModel = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e781d9-3908-48fd-8e65-ab3b2692a5da",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940e432-5338-464e-8671-88087fb74e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data (Prediction)\n",
    "# predictions = model_lr.transform(testData)\n",
    "predictions = rfModel.transform(test_data)\n",
    "# Display the predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dd03b-71f9-4795-90b1-78cee9090d49",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e1c34-cdae-418c-afda-429aef3dbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# # Evaluate the performance of the model\n",
    "# evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "lrEval3 = BinaryClassificationEvaluator(labelCol='label')\n",
    "# evaluator2_area_ROC = lrEval.evaluate(predictions)\n",
    "auroc3 = lrEval3.evaluate(predictions,{lrEval3.metricName:'areaUnderROC'})\n",
    "aupr3 = lrEval3.evaluate(predictions,{lrEval3.metricName:'areaUnderPR'})\n",
    "\n",
    "print(f\"AUROC: {auroc3}\")\n",
    "print(f\"AUPR: {aupr3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3f5b3-a9a2-43a6-9a56-42d178c2769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa547a-c6b1-4dc9-9d4b-6a184f89b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MulticlassClassificationEvaluator\n",
    "evaluator3 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute accuracy, precision, recall, and F1 score\n",
    "accuracy3 = evaluator3.evaluate(predictions, {evaluator3.metricName: \"accuracy\"})\n",
    "precision3 = evaluator3.evaluate(predictions, {evaluator3.metricName: \"weightedPrecision\"})\n",
    "recall3 = evaluator3.evaluate(predictions, {evaluator3.metricName: \"weightedRecall\"})\n",
    "f13 = evaluator3.evaluate(predictions, {evaluator3.metricName: \"f1\"})\n",
    "\n",
    "# Print the metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc9a23-6814-4ae7-ac67-34ef7b766483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns confusion matrix: predicted classes are in columns, they are ordered by class label ascending, as in “labels”.\n",
    "# Meaning 0 1 in columns. Rows also 0 then 1.\n",
    "\n",
    "# Evaluate using MultiClass Metrics\n",
    "pred_rdd = predictions.select('prediction','label').rdd.map(tuple)\n",
    "metrics = MulticlassMetrics(pred_rdd)\n",
    "\n",
    "print(metrics.confusionMatrix())\n",
    "\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "labels = ['Class 0', 'Class 1']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "# Add actual values to the cells\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, str(int(confusion_matrix[i, j])), fontsize=12, ha='center', va='center', color='red')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8238e-f36e-48f0-8210-6d3364e3ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Evaluate using MultiClass Metrics\n",
    "# pred_rdd = predictions.select('prediction','label').rdd.map(tuple)\n",
    "# metrics = MulticlassMetrics(pred_rdd)\n",
    "# accuracy_o = metrics.accuracy   # Positive class\n",
    "# precision_o = metrics.precision(1.0)  # Positive class\n",
    "# recall_o = metrics.recall(1.0)  # Positive class\n",
    "# f1_o = metrics.fMeasure(1.0)  # Positive class\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_o)\n",
    "# print(\"Precision:\", precision_o)\n",
    "# print(\"Recall:\", recall_o)\n",
    "# print(\"F1-score:\", f1_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fbaa6-8b32-44eb-b5c3-3358f06c8fff",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd648f-441d-4400-a920-c7d5b0b6c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f61d9d-a09b-45f8-843c-e742d256e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "# grid = grid.addGrid(\n",
    "#                     model_lr.aggregationDepth, [2, 3, 4])\\\n",
    "#                     .addGrid(model_lr.regParam, np.logspace(1e-3,1e-1)\n",
    "#                     )\\\n",
    "#                     .build()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator = rf, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = lrEval3,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a1522-3c74-4650-817d-84551316143e",
   "metadata": {},
   "source": [
    "## Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14784ec8-d820-4e48-bc37-e5496ee6db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model3 = bestModel\n",
    "pprint(model3.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b3a2-3959-4560-8d77-760593700114",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5da225-6b4b-48b2-9449-313abc6e816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.write().overwrite().save(\"project/models/model3\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model3 models/model3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13255877-5d1f-412c-a727-4372018ca076",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35671c1b-cdc4-4c08-a32f-0278d3881b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model3.transform(test_data)\n",
    "predictions.show()\n",
    "\n",
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model3_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model3_predictions.csv/*.csv > output/model3_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b02fb-2bfd-4563-83db-e0b544f821fa",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d91af-49a2-49c6-b8a1-2dc1dfdad9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse1 = evaluator1.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b75c5-8835-450c-979f-1e346939f343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22bd7b48-f536-495a-ac38-90ea1b9014af",
   "metadata": {},
   "source": [
    "# Compare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91f3c3-50ca-4ce1-9dfa-dba7449fbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[str(model1),auroc1, aupr1, f11], [str(model2),auroc2, aupr2, f12], [str(model3),auroc3, aupr3, f13]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"AUROC\", \"AUPR\", \"F1\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008d30b-9489-44a2-84a0-95bcd9608319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc343753-cfc9-44b0-8aa9-877c14d9f750",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
